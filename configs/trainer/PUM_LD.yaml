# configs/trainer/PUM_LD.yaml
name: PUM_LD
handler: PUM_LD     # <-- this is what load_trainer(..) uses

pum_cfg:            # forwarded to the handler as method_args.pum_cfg (via load_trainer)
  m: 6
  alpha: [1.0, 1.5, 2.0, 3.0, 4.0, 6.0]
  sigma_mode: rms_kappa
  sigma_fixed: 0.05
  kappa: 0.10
  eta_srv: 1.0
  seed_train: 0
  seed_noise: 17
  seed_reparam: 23
  reparam_attention_rotate: true
  reparam_ffn_pair_permute: true
  reparam_residual_permute: false
  attn_num_heads: null
  attn_num_kv_heads: null 
  verbose: true

# Optional: local client steps & lr
method_args:
  local_steps: 10
  local_lr: 1e-4

# HF Trainer args (your script overrides these as usual)
args:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  ddp_find_unused_parameters: true
  gradient_checkpointing: true
  do_train: True
